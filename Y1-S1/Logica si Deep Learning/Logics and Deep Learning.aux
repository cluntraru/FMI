\relax 
\citation{NTN}
\citation{LTN}
\citation{Deep_Learning}
\citation{Deep_Learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks}{1}}
\citation{Deep_Learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neural network and neuron \cite  {Deep_Learning}}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Workings of a neuron}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neuron \cite  {Deep_Learning}}}{2}}
\citation{Deep_Learning}
\citation{Deep_Learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Neural net for logical AND \cite  {Deep_Learning}}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Truth table for AND and $\sigma $ \cite  {Deep_Learning}}}{3}}
\citation{Deep_Learning}
\citation{Deep_Learning}
\citation{Deep_Learning}
\citation{Deep_Learning}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Neural net for XNOR \cite  {Deep_Learning}}}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backpropagation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Partial derivative in a neural net \cite  {Deep_Learning}}}{4}}
\citation{Tensor}
\citation{Tensor}
\citation{Tensor_Rank}
\citation{Tensor_Multiplication}
\@writefile{toc}{\contentsline {section}{\numberline {3}Tensor}{5}}
\citation{NTN}
\citation{NTN}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Tensor Networks}{6}}
\citation{NTN}
\citation{NTN}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Graphical representation of the formula \cite  {NTN}}}{7}}
\citation{FOL}
\citation{FOL}
\citation{Atomic_Formula}
\citation{Literal}
\citation{Clause}
\citation{Clause}
\citation{Closed_Term}
\citation{Free_Variable}
\citation{Free_Variable}
\citation{LTN}
\@writefile{toc}{\contentsline {section}{\numberline {5}A short presentation of first order logic}{8}}
\citation{LTN}
\citation{LTN}
\citation{NTN}
\citation{NTN}
\citation{LTN}
\citation{LTN}
\@writefile{toc}{\contentsline {section}{\numberline {6}Logic Tensor Network}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Real Logic}{9}}
\citation{LTN}
\citation{FOL}
\citation{FOL}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Learning as approximate satisfiability}{12}}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\citation{NTN}
\citation{LTN}
\@writefile{toc}{\contentsline {section}{\numberline {7}Real Logic in Neural Tensor Networks}{13}}
\citation{LTN}
\citation{LTN}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Tensor network for $\neg P(x, y) \rightarrow A(y)$, where $\mathcal  {G}(x) = v$, $\mathcal  {G}(y) = u$ si $k = 2$ \cite  {LTN}}}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {8}An example of knowledge base completion}{14}}
\citation{LTN}
\citation{LTN}
\citation{LTN}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Initial knowledge base \cite  {LTN}}}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Knowledge base completed by ``ltn'' \cite  {LTN}}}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Personal thoughts}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Questions raised by the model}{17}}
\bibcite{LTN}{1}
\bibcite{NTN}{2}
\bibcite{Tensor}{3}
\bibcite{Tensor_Rank}{4}
\bibcite{Tensor_Multiplication}{5}
\bibcite{Deep_Learning}{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Possible uses for Logic Tensor Networks}{18}}
\bibcite{FOL}{7}
\bibcite{Clause}{8}
\bibcite{Closed_Term}{9}
\bibcite{Free_Variable}{10}
\bibcite{Atomic_Formula}{11}
\bibcite{Literal}{12}
